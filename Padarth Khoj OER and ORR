import tkinter as tk
from tkinter import filedialog, messagebox, ttk
import os
import shutil
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression # Import LinearRegression
from sklearn.metrics import r2_score


# === MODEL TRAINING SECTION


# Dictionary to store the trained models and their scalers
trained_models = {}

def train_oer_model():
    """Trains and stores the SVR model for OER analysis."""
    try:
        df_train = pd.read_csv("oer.csv")
        x_train_orig = df_train.iloc[:, 0:4].values
        y_train_orig = df_train.iloc[:, 4].values.reshape(-1, 1)

        sc_X = StandardScaler()
        X_train_scaled = sc_X.fit_transform(x_train_orig)
        sc_y = StandardScaler()
        y_train_scaled = sc_y.fit_transform(y_train_orig)

        regressor_svr = SVR(kernel='linear')
        regressor_svr.fit(X_train_scaled, y_train_scaled.ravel())
        trained_models['OER_SVR'] = {
            'model': regressor_svr,
            'scaler_X': sc_X,
            'scaler_y': sc_y
        }
        print("OER SVR model trained successfully.")
    except FileNotFoundError:
        messagebox.showerror("Error", "Training file 'oer_sama.csv' not found.")
    except Exception as e:
        messagebox.showerror("Error", f"An error occurred during OER SVR model training: {e}")

def train_orr_model():
    """Trains and stores the SVR model for ORR analysis."""
    try:
        df_train = pd.read_csv("orr.csv")
        x_train_orig = df_train.iloc[:, 0:2].values
        y_train_orig = df_train.iloc[:, 2].values.reshape(-1, 1)

        sc_X = StandardScaler()
        X_train_scaled = sc_X.fit_transform(x_train_orig)
        sc_y = StandardScaler()
        y_train_scaled = sc_y.fit_transform(y_train_orig)

        regressor_svr = SVR(kernel='linear')
        regressor_svr.fit(X_train_scaled, y_train_scaled.ravel())
        trained_models['ORR_SVR'] = {
            'model': regressor_svr,
            'scaler_X': sc_X,
            'scaler_y': sc_y
        }
        print("ORR SVR model trained successfully.")
    except FileNotFoundError:
        messagebox.showerror("Error", "Training file 'orr.csv' not found.")
    except Exception as e:
        messagebox.showerror("Error", f"An error occurred during ORR SVR model training: {e}")

def train_oer_mlr_model():
    """Trains and stores the MLR model for OER analysis."""
    try:
        df_train = pd.read_csv("oer_sama.csv")
        x_train_orig = df_train.iloc[:, 0:4].values
        y_train_orig = df_train.iloc[:, 4].values.reshape(-1, 1)

        sc_X = StandardScaler()
        X_train_scaled = sc_X.fit_transform(x_train_orig)
        sc_y = StandardScaler()
        y_train_scaled = sc_y.fit_transform(y_train_orig)

        regressor_mlr = LinearRegression()
        regressor_mlr.fit(X_train_scaled, y_train_scaled.ravel())
        trained_models['OER_MLR'] = {
            'model': regressor_mlr,
            'scaler_X': sc_X,
            'scaler_y': sc_y
        }
        print("OER MLR model trained successfully.")
    except FileNotFoundError:
        messagebox.showerror("Error", "Training file 'oer_sama.csv' not found for MLR.")
    except Exception as e:
        messagebox.showerror("Error", f"An error occurred during OER MLR model training: {e}")

def train_orr_mlr_model():
    """Trains and stores the MLR model for ORR analysis."""
    try:
        df_train = pd.read_csv("orr.csv")
        x_train_orig = df_train.iloc[:, 0:2].values
        y_train_orig = df_train.iloc[:, 2].values.reshape(-1, 1)

        sc_X = StandardScaler()
        X_train_scaled = sc_X.fit_transform(x_train_orig)
        sc_y = StandardScaler()
        y_train_scaled = sc_y.fit_transform(y_train_orig)

        regressor_mlr = LinearRegression()
        regressor_mlr.fit(X_train_scaled, y_train_scaled.ravel())
        trained_models['ORR_MLR'] = {
            'model': regressor_mlr,
            'scaler_X': sc_X,
            'scaler_y': sc_y
        }
        print("ORR MLR model trained successfully.")
    except FileNotFoundError:
        messagebox.showerror("Error", "Training file 'orr.csv' not found for MLR.")
    except Exception as e:
        messagebox.showerror("Error", f"An error occurred during ORR MLR model training: {e}")


# === CORE ANALYSIS FUNCTIONS


def read_contcar(filename):
    """
    Parse a CONTCAR file to extract lattice vectors, fractional coordinates,
    and 0-based carbon atom indices.
    """
    try:
        with open(filename, 'r') as f:
            lines = f.readlines()
    except FileNotFoundError:
        print(f"Error: CONTCAR file not found at {filename}")
        return None, None, None
    if len(lines) < 8:
        print(f"Error: CONTCAR file {filename} seems too short.")
        return None, None, None

    try:
        scaling_factor = float(lines[1].strip())
        lattice = np.array([list(map(float, lines[i].split())) for i in range(2, 5)]) * scaling_factor
        atom_symbols = lines[5].split()
        natoms = list(map(int, lines[6].split()))
        total_atoms = sum(natoms)

        try:
            c_symbol_index = atom_symbols.index('C')
        except ValueError:
            print("Error: 'C' (Carbon) not found in atom symbols line:", lines[5].strip())
            return None, None, None

        start_indices = [0] + list(np.cumsum(natoms[:-1]))
        carbon_start_index = start_indices[c_symbol_index]
        num_carbon = natoms[c_symbol_index]
        carbon_indices = list(range(carbon_start_index, carbon_start_index + num_carbon))

        line_index = 7
        if lines[line_index].strip().lower().startswith('s'):
            line_index += 1
        coord_type_line = lines[line_index].strip().lower()
        is_direct = coord_type_line.startswith('d') or coord_type_line.startswith('f')

        coord_start_line = line_index + 1
        if coord_start_line + total_atoms > len(lines):
            print(f"Error: Not enough coordinate lines in {filename}. Expected {total_atoms}, found {len(lines) - coord_start_line}.")
            return None, None, None

        coords = []
        coord_lines = lines[coord_start_line:coord_start_line + total_atoms]
        for i, line in enumerate(coord_lines):
            parts = line.split()
            if len(parts) >= 3:
                coords.append(list(map(float, parts[:3])))
            else:
                print(f"Error: Malformed coordinate line {coord_start_line + i + 1} in {filename}: {line.strip()}")
                return None, None, None
        coords = np.array(coords)

        if not is_direct:
            inv_lattice = np.linalg.inv(lattice)
            frac_coords = np.dot(coords, inv_lattice)
        else:
            frac_coords = coords

        frac_coords = frac_coords % 1.0

    except (ValueError, IndexError) as e:
        print(f"Error parsing CONTCAR file {filename}: {e}")
        return None, None, None

    return lattice, coords, carbon_indices

def min_image_distance(frac1, frac2, lattice):
    """
    Calculate the minimum image distance between two atoms under periodic boundary conditions.
    """
    df = frac2 - frac1
    df -= np.rint(df)
    dc = np.dot(df, lattice)
    return np.linalg.norm(dc)

def find_adjacent_pairs(frac_coords, carbon_indices, lattice, cutoff=1.6):
    """
    Identify pairs of adjacent carbon atoms based on a distance cutoff.
    """
    if frac_coords is None or carbon_indices is None or lattice is None:
        print("Error: Invalid input to find_adjacent_pairs.")
        return None
    if cutoff <= 0:
        print("Warning: Cutoff distance must be positive.")
        return []

    pairs = []
    num_carbon_atoms = len(carbon_indices)
    for i in range(num_carbon_atoms):
        for j in range(i + 1, num_carbon_atoms):
            idx1 = carbon_indices[i]
            idx2 = carbon_indices[j]
            if idx1 >= len(frac_coords) or idx2 >= len(frac_coords):
                print(f"Warning: Carbon index out of bounds ({idx1} or {idx2} >= {len(frac_coords)}). Skipping pair.")
                continue
            dist = min_image_distance(frac_coords[idx1], frac_coords[idx2], lattice)
            if dist < cutoff:
                pairs.append((idx1, idx2))
    return pairs

def split_doscar(doscar_path, output_folder):
    """
    Splits a VASP DOSCAR file into individual DOS files, formatted to match the HPC script.
    """
    outcar_path = os.path.join(output_folder, 'OUTCAR')
    if not os.path.exists(outcar_path):
        messagebox.showerror("Error", f"OUTCAR file not found at: {outcar_path}")
        return 0, "Error: OUTCAR not found."
    
    # Read EFERMI and ISPIN from OUTCAR
    efermi = 0.0
    nspin = 1
    try:
        with open(outcar_path, 'r') as f:
            for line in f:
                if 'E-fermi' in line:
                    efermi = float(line.split()[2])
                if 'ISPIN' in line:
                    nspin = int(line.split()[2])
    except Exception as e:
        messagebox.showerror("Error", f"Could not parse OUTCAR for EFERMI/ISPIN: {e}")
        return 0, "Error parsing OUTCAR."
    
    # Read DOSCAR header
    with open(doscar_path, 'r') as f:
        lines = f.readlines()
    
    try:
        natom = int(lines[0].split()[0])
        nedos = int(lines[5].split()[2])
    except (ValueError, IndexError):
        return 0, "Error: Could not parse DOSCAR header."

    current_line = 6 + nedos
    
    # Process atomic projected DOS
    positions_info = read_contcar(os.path.join(output_folder, 'CONTCAR'))
    if positions_info is None:
        return 0, "Error: Could not read CONTCAR for atomic positions."

    positions = positions_info[1] # The second return value is the list of coordinates
    
    atom_index = 1
    split_count = 0
    while current_line < len(lines):
        if current_line + nedos > len(lines):
            break

        atomic_dos_lines = lines[current_line + 1:current_line + 1 + nedos]
        
        dos_file_path = os.path.join(output_folder, f'DOS{atom_index}')
        with open(dos_file_path, 'w') as outf:
            if positions is not None and len(positions) >= atom_index:
                pos = positions[atom_index - 1]
                # Corrected format string for writing individual elements of the NumPy array
                outf.write(f"# {pos[0]:12.8f} {pos[1]:12.8f} {pos[2]:12.8f}\n")
            
            for line in atomic_dos_lines:
                fields = line.split()
                # Apply EFERMI shift
                energy = float(fields[0]) - efermi
                if nspin == 2:
                    # Case for ISPIN = 2 (spin-polarized)
                    output = [energy]
                    for j in range(1, 10):
                        up = float(fields[2 * j - 1])
                        down = -float(fields[2 * j])
                        output.extend([up, down])
                    outf.write(" ".join(f"{val:12.8f}" for val in output) + "\n")
                else:
                    # Case for ISPIN = 1 (non-spin-polarized)
                    output = [energy] + [float(fields[j]) for j in range(1, 10)]
                    outf.write(" ".join(f"{val:12.8f}" for val in output) + "\n")
        
        current_line += 1 + nedos
        atom_index += 1
        split_count += 1

    if natom != split_count:
        return split_count, f"Warning: Expected {natom} atoms, but split {split_count} PDOS files."
        
    return natom, None


# Function to read DOS files
def read_dos_files(folder_path, n):
    """Reads DOS files (DOS1 to DOSn) from the specified folder."""
    dos_files = []
    for i in range(1, n + 1):
        dos_file_name = f'DOS{i}'
        dos_file_path = os.path.join(folder_path, dos_file_name)
        if os.path.isfile(dos_file_path):
            dos_files.append(dos_file_path)
    return dos_files

# Function to calculate overpotential (ORR)
def calculate_overpotential_orr(del_goh):
    """Calculates overpotential based on ORR Del_Goh value."""
    if del_goh < 0.13914:
        return 0.3672 - 0.8268 * del_goh
    else:
        return 0.11252 + 0.98707 * del_goh
        
# Function to calculate overpotential (OER)
def calculate_overpotential_oer(del_goh_minus_del_go):
    """
    Calculate overpotential based on OER DelGoh_minus_DelGo value.
    """
    if del_goh_minus_del_go < 2.31212:
        return 2.3612 - 0.6811 * del_goh_minus_del_go
    else:
        return 2.032 + 1 * del_goh_minus_del_go

def compute_pz_parameters(dos_file):
    """
    Compute pz_fermi and pz_occ from a VASP DOS file using p_z orbital data.
    Assumes Fermi level is at 0 eV.
    """
    try:
        dos_data = pd.read_csv(dos_file, header=None, delim_whitespace=True, comment='#')
        pz_up_col = 5
        pz_down_col = 6
        
        if dos_data.shape[1] <= max(pz_up_col, pz_down_col):
            return 0.0, 0.0, f"Error: File '{os.path.basename(dos_file)}' has insufficient columns for PDOS analysis."

        energy = dos_data.iloc[:, 0].values
        dos_pz_up = dos_data.iloc[:, pz_up_col].values
        dos_pz_down = dos_data.iloc[:, pz_down_col].values
        pz = dos_pz_up - dos_pz_down

        negative_mask = energy < 0
        if np.any(negative_mask):
            neg_indices = np.where(negative_mask)[0]
            if neg_indices.size > 0:
                idx_of_max_neg_energy = neg_indices[np.argmax(energy[neg_indices])]
                pz_fermi = pz[idx_of_max_neg_energy]
                sort_indices = np.argsort(energy[negative_mask])
                pz_occ = np.trapz(pz[negative_mask][sort_indices], energy[negative_mask][sort_indices])
            else:
                return 0.0, 0.0, f"Warning: File '{os.path.basename(dos_file)}' has no negative energy points."
        else:
            return 0.0, 0.0, f"Warning: File '{os.path.basename(dos_file)}' has no energy points below 0 eV."
        return pz_fermi, pz_occ, None
    except FileNotFoundError:
        return 0.0, 0.0, f"Error: DOS file '{os.path.basename(dos_file)}' not found."
    except pd.errors.EmptyDataError:
        return 0.0, 0.0, f"Error: DOS file '{os.path.basename(dos_file)}' is empty or unreadable."
    except Exception as e:
        return 0.0, 0.0, f"Error processing file '{os.path.basename(dos_file)}': {e}"

def run_ml_analysis(folder_path, n_atoms, reaction_type):
    """
    Performs the ML analysis based on the specified reaction type (OER or ORR).
    """
    model_method = ml_method_var.get()
    
    if model_method == "SVR":
        model_key = reaction_type + "_SVR"
        if model_key not in trained_models:
            messagebox.showerror("Error", f"Training model for '{model_key}' not found. Please ensure training data exists.")
            return None, None, None
        model_data = trained_models[model_key]
        model = model_data['model']
        scaler_X = model_data['scaler_X']
        scaler_y = model_data['scaler_y']
        
        if reaction_type == "ORR":
            return analyze_orr_ml(folder_path, n_atoms, model, scaler_X, scaler_y)
        elif reaction_type == "OER":
            return analyze_oer_ml(folder_path, n_atoms, model, scaler_X, scaler_y)
            
    elif model_method == "MLR":
        model_key = reaction_type + "_MLR"
        if model_key not in trained_models:
            messagebox.showerror("Error", f"Training model for '{model_key}' not found. Please ensure training data exists.")
            return None, None, None
        model_data = trained_models[model_key]
        model = model_data['model']
        scaler_X = model_data['scaler_X']
        scaler_y = model_data['scaler_y']
        
        if reaction_type == "ORR":
            return analyze_orr_ml(folder_path, n_atoms, model, scaler_X, scaler_y, method="MLR")
        elif reaction_type == "OER":
            return analyze_oer_ml(folder_path, n_atoms, model, scaler_X, scaler_y, method="MLR")

def analyze_orr_ml(folder_path, n, model, sc_X, sc_y, method="SVR"):
    """Performs ORR analysis using an SVR or MLR model."""
    dos_files = [os.path.join(folder_path, f'DOS{i}') for i in range(1, n + 1) if os.path.exists(os.path.join(folder_path, f'DOS{i}'))]
    if not dos_files:
        return "No DOS files found.", None, None

    # Corrected DataFrame columns
    results_df = pd.DataFrame(columns=['File', 'Pz_Occupancy', 'Pz_at_Fermi', 'r_Occupancy', 'Del_Goh', 'Overpotential'])
    warnings = []
    
    for dos_file in dos_files:
        pz_fermi, pz_occ, err = compute_pz_parameters(dos_file)
        if err:
            warnings.append(f"Skipping {os.path.basename(dos_file)} due to an error: {err}")
            continue
        
        # Calculate the new input feature r_occ
        r_occ = pz_occ - 0.47064

        # The input features for the SVR model are r_occ and pz_fermi
        input_features = np.array([[r_occ, pz_fermi]])
        scaled_features = sc_X.transform(input_features)
        
        predicted_del_goh_scaled = model.predict(scaled_features)
        del_goh = sc_y.inverse_transform(predicted_del_goh_scaled.reshape(-1, 1))[0][0]

        overpotential = calculate_overpotential_orr(del_goh)
        
        # Correctly insert data into the DataFrame
        results_df.loc[len(results_df)] = [dos_file, pz_occ, pz_fermi, r_occ, del_goh, overpotential]
    
    if results_df.empty:
        return "No valid DOS files were processed.", None, None

    output_str = f"Analysis for ORR using {method}\n\n"
    if warnings:
        output_str += "--- Warnings ---\n" + "\n".join(warnings) + "\n\n"
    output_str += results_df[['File', 'Del_Goh', 'Overpotential']].to_string()
    
    excel_file_path = os.path.join(folder_path, 'dos_results.xlsx')
    results_df.to_excel(excel_file_path, index=False)
    
    csv_file_path = os.path.join(folder_path, f'overpotentialsORR_{method}.csv')
    results_df.to_csv(csv_file_path, index=False)
    
    return output_str, excel_file_path, csv_file_path

def analyze_oer_ml(folder_path, n_atoms, model, sc_X, sc_y, method="SVR"):
    """
    Performs OER analysis using an SVR or MLR model and a CONTCAR file.
    """
    contcar_file = os.path.join(folder_path, 'CONTCAR')
    lattice, coords, carbon_indices = read_contcar(contcar_file)
    if lattice is None or coords is None or carbon_indices is None:
        return "Error: CONTCAR file not found or could not be parsed.", None, None
    
    pairs_0based = find_adjacent_pairs(coords, carbon_indices, lattice, cutoff=1.6)
    if pairs_0based is None:
        return "Error: Could not find adjacent carbon pairs.", None, None

    s_params_by_list_idx = {}
    
    # Get a list of original atom indices that have valid DOS files
    existing_dos_indices = [idx for idx in carbon_indices if os.path.exists(os.path.join(folder_path, f'DOS{idx + 1}'))]

    # Process only the existing PDOS files for carbon atoms
    for original_idx in existing_dos_indices:
        dos_file = os.path.join(folder_path, f'DOS{original_idx + 1}')
        pz_fermi_s, pz_occ_s, err = compute_pz_parameters(dos_file)
        if not err:
            # Calculate the new r_occ_s parameter
            r_occ_s = pz_occ_s - 0.47064
            s_params_by_list_idx[original_idx] = {'pz_fermi_s': pz_fermi_s, 'pz_occ_s': pz_occ_s, 'r_occ_s': r_occ_s}

    if not s_params_by_list_idx:
        return "No valid DOS files for carbon atoms were processed.", None, None

    # Map original indices to a new list for adjacency list creation
    original_idx_to_list_idx = {orig_idx: list_idx for list_idx, orig_idx in enumerate(carbon_indices)}
    adj_list = {i: [] for i in range(len(carbon_indices))}
    
    # Build the adjacency list for existing carbon atoms
    for idx1_orig, idx2_orig in pairs_0based:
        if idx1_orig in original_idx_to_list_idx and idx2_orig in original_idx_to_list_idx:
            list_idx1 = original_idx_to_list_idx[idx1_orig]
            list_idx2 = original
